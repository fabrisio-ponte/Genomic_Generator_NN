{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: DNA RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.3872\n",
      "Epoch 500, Loss: 1.3531\n",
      "Epoch 1000, Loss: 1.3394\n",
      "Epoch 1500, Loss: 1.3391\n",
      "Epoch 2000, Loss: 1.3326\n",
      "Epoch 2500, Loss: 1.3307\n",
      "Epoch 3000, Loss: 1.3230\n",
      "Epoch 3500, Loss: 1.3147\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# ------------------ Load and preprocess the genome ------------------\n",
    "# Load file\n",
    "with open(\"Ecoli_GCF_003018035.1_ASM301803v1_genomic.fna\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Clean sequence (remove FASTA headers and newlines)\n",
    "sequence = ''.join([line.strip() for line in lines if not line.startswith('>')])\n",
    "\n",
    "# Vocabulary mapping\n",
    "vocab = ['A', 'C', 'G', 'T']\n",
    "char_to_idx = {ch: idx for idx, ch in enumerate(vocab)}\n",
    "idx_to_char = {idx: ch for ch, idx in char_to_idx.items()}\n",
    "\n",
    "# Encode the sequence\n",
    "encoded_seq = np.array([char_to_idx[ch] for ch in sequence if ch in char_to_idx], dtype=np.int64)\n",
    "\n",
    "# Train-test split\n",
    "split_idx = int(len(encoded_seq) * 0.8)\n",
    "train_data = encoded_seq[:split_idx]\n",
    "test_data = encoded_seq[split_idx:]\n",
    "\n",
    "# ------------------ PyTorch Dataset ------------------\n",
    "class DNADataset(Dataset):\n",
    "    def __init__(self, data, seq_length):\n",
    "        self.data = data\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.seq_length]\n",
    "        y = self.data[idx + 1:idx + self.seq_length + 1]\n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Parameters\n",
    "seq_length = 100\n",
    "batch_size = 64\n",
    "\n",
    "# DataLoaders\n",
    "train_ds = DNADataset(train_data, seq_length)\n",
    "test_ds = DNADataset(test_data, seq_length)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ------------------ RNN Model ------------------\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        out = self.embedding(x)  # shape: (batch, seq_len, embed_dim)\n",
    "        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
    "        out = self.fc(out)\n",
    "        return out, hidden, cell\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size).to(device)\n",
    "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size).to(device)\n",
    "        return hidden, cell\n",
    "\n",
    "# ------------------ Training ------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 256\n",
    "rnn_hidden_size = 512\n",
    "\n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "num_epochs = 4000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    hidden, cell = model.init_hidden(batch_size, device)\n",
    "    seq_batch, target_batch = next(iter(train_dl))\n",
    "    seq_batch = seq_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    for c in range(seq_length):\n",
    "        pred, hidden, cell = model(seq_batch[:, c].unsqueeze(1), hidden, cell)\n",
    "        pred = pred.squeeze(1)\n",
    "        loss += loss_fn(pred, target_batch[:, c])\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss = loss.item() / seq_length\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.3236\n"
     ]
    }
   ],
   "source": [
    "# ------------------ Testing ------------------\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for seq_batch, target_batch in test_dl:\n",
    "        seq_batch, target_batch = seq_batch.to(device), target_batch.to(device)\n",
    "        hidden, cell = model.init_hidden(seq_batch.size(0), device)\n",
    "        output, _, _ = model(seq_batch, hidden, cell)\n",
    "        \n",
    "        # Reshape for loss\n",
    "        output = output.view(-1, vocab_size)\n",
    "        target_batch = target_batch.view(-1)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_fn(output, target_batch)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "avg_test_loss = test_loss / len(test_dl)\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: DNA (k-mer) with RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 5.5505\n",
      "Epoch 100, Loss: 1.3877\n",
      "Epoch 200, Loss: 1.3642\n",
      "Epoch 300, Loss: 1.3702\n",
      "Epoch 400, Loss: 1.3568\n",
      "Epoch 500, Loss: 1.3513\n",
      "Epoch 600, Loss: 1.3565\n",
      "Epoch 700, Loss: 1.3562\n",
      "Epoch 800, Loss: 1.3688\n",
      "Epoch 900, Loss: 1.3418\n",
      "Test Loss: 1.3606\n",
      "\n",
      "Initial sequence: TGCTCTCTTTACCGCTGTTT\n",
      "Predicted next k-mers: ['TTTT', 'TTTT', 'TTTC', 'TTCA', 'TCAG']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# ------------------ Load and preprocess the genome ------------------\n",
    "# Load file\n",
    "with open(\"Ecoli_GCF_003018035.1_ASM301803v1_genomic.fna\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Clean sequence (remove FASTA headers and newlines)\n",
    "sequence = ''.join([line.strip() for line in lines if not line.startswith('>')])\n",
    "\n",
    "# Function to generate k-mers\n",
    "def generate_kmers(sequence, k=4):\n",
    "    kmers = [sequence[i:i+k] for i in range(len(sequence) - k + 1)]\n",
    "    return kmers\n",
    "\n",
    "# Generate k-mers\n",
    "k = 4  # You can change this to 3\n",
    "kmers = generate_kmers(sequence, k)\n",
    "\n",
    "# Create vocabulary of k-mers\n",
    "kmer_counts = Counter(kmers)\n",
    "vocab = list(kmer_counts.keys())\n",
    "char_to_idx = {kmer: idx for idx, kmer in enumerate(vocab)}\n",
    "idx_to_char = {idx: kmer for kmer, idx in char_to_idx.items()}\n",
    "\n",
    "# Encode the sequence of k-mers\n",
    "encoded_seq = np.array([char_to_idx[kmer] for kmer in kmers], dtype=np.int64)\n",
    "\n",
    "# Train-test split\n",
    "split_idx = int(len(encoded_seq) * 0.8)\n",
    "train_data = encoded_seq[:split_idx]\n",
    "test_data = encoded_seq[split_idx:]\n",
    "\n",
    "# ------------------ PyTorch Dataset ------------------\n",
    "class KmerDataset(Dataset):\n",
    "    def __init__(self, data, seq_length):\n",
    "        self.data = data\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.seq_length]\n",
    "        y = self.data[idx + 1:idx + self.seq_length + 1]\n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Parameters\n",
    "seq_length = 50  # Number of k-mers in a sequence\n",
    "batch_size = 64\n",
    "\n",
    "# DataLoaders\n",
    "train_ds = KmerDataset(train_data, seq_length)\n",
    "test_ds = KmerDataset(test_data, seq_length)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ------------------ RNN Model ------------------\n",
    "class KmerRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        out = self.embedding(x)  # shape: (batch, seq_len, embed_dim)\n",
    "        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
    "        out = self.fc(out)\n",
    "        return out, hidden, cell\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size).to(device)\n",
    "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size).to(device)\n",
    "        return hidden, cell\n",
    "\n",
    "# ------------------ Training ------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 256\n",
    "rnn_hidden_size = 512\n",
    "\n",
    "model = KmerRNN(vocab_size, embed_dim, rnn_hidden_size).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    hidden, cell = model.init_hidden(batch_size, device)\n",
    "    seq_batch, target_batch = next(iter(train_dl))\n",
    "    seq_batch = seq_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    for c in range(seq_length):\n",
    "        pred, hidden, cell = model(seq_batch[:, c].unsqueeze(1), hidden, cell)\n",
    "        pred = pred.squeeze(1)\n",
    "        loss += loss_fn(pred, target_batch[:, c])\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss = loss.item() / seq_length\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# ------------------ Testing ------------------\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for seq_batch, target_batch in test_dl:\n",
    "        seq_batch, target_batch = seq_batch.to(device), target_batch.to(device)\n",
    "        hidden, cell = model.init_hidden(seq_batch.size(0), device)\n",
    "        output, _, _ = model(seq_batch, hidden, cell)\n",
    "        \n",
    "        # Reshape for loss\n",
    "        output = output.view(-1, vocab_size)\n",
    "        target_batch = target_batch.view(-1)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_fn(output, target_batch)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "avg_test_loss = test_loss / len(test_dl)\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "# ------------------ Prediction Example ------------------\n",
    "def predict_next_kmer(model, initial_sequence, k=4, num_predictions=5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Convert initial sequence to k-mers\n",
    "        initial_kmers = generate_kmers(initial_sequence, k)\n",
    "        # Convert to indices\n",
    "        input_indices = [char_to_idx[kmer] for kmer in initial_kmers if kmer in char_to_idx]\n",
    "        \n",
    "        if not input_indices:\n",
    "            return \"No valid k-mers found in initial sequence\"\n",
    "            \n",
    "        # Get the true next k-mers from the sequence\n",
    "        true_next_kmers = generate_kmers(initial_sequence[k:], k)[:num_predictions]\n",
    "        \n",
    "        # Convert to tensor\n",
    "        input_tensor = torch.tensor(input_indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        hidden, cell = model.init_hidden(1, device)\n",
    "        \n",
    "        predictions = []\n",
    "        for _ in range(num_predictions):\n",
    "            output, hidden, cell = model(input_tensor, hidden, cell)\n",
    "            predicted_idx = output[:, -1].argmax(1).item()\n",
    "            predictions.append(idx_to_char[predicted_idx])\n",
    "            input_tensor = torch.cat([input_tensor, torch.tensor([[predicted_idx]], dtype=torch.long).to(device)], dim=1)\n",
    "            \n",
    "    return predictions, true_next_kmers\n",
    "\n",
    "# Example usage\n",
    "test_sequence = sequence[:20]  # Take first 20 nucleotides\n",
    "predictions, true_next = predict_next_kmer(model, test_sequence)\n",
    "print(\"\\nInitial sequence:\", test_sequence)\n",
    "print(\"Predicted next k-mers:\", predictions)\n",
    "print(\"True next k-mers:\", true_next)\n",
    "print(\"\\nComparison:\")\n",
    "for i, (pred, true) in enumerate(zip(predictions, true_next)):\n",
    "    print(f\"Position {i+1}: Predicted: {pred}, True: {true}, Match: {pred == true}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: DNA (K-mers) with RNN and Attention Mechanism Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 5.5496\n",
      "Epoch 100, Loss: 1.3059\n",
      "Epoch 200, Loss: 1.1240\n",
      "Epoch 300, Loss: 1.0030\n",
      "Epoch 400, Loss: 0.8790\n",
      "Epoch 500, Loss: 0.8197\n",
      "Epoch 600, Loss: 0.7271\n",
      "Epoch 700, Loss: 0.6735\n",
      "Epoch 800, Loss: 0.6122\n",
      "Epoch 900, Loss: 0.5806\n",
      "Test Loss: 0.5421\n",
      "\n",
      "Initial sequence: TGCTCTCTTTACCGCTGTTT\n",
      "Predicted next k-mers: ['TTTC', 'TTCT', 'TCTC', 'CTCT', 'TCTC']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# ------------------ Load and preprocess the genome ------------------\n",
    "# Load file\n",
    "with open(\"Ecoli_GCF_003018035.1_ASM301803v1_genomic.fna\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Clean sequence (remove FASTA headers and newlines)\n",
    "sequence = ''.join([line.strip() for line in lines if not line.startswith('>')])\n",
    "\n",
    "# Function to generate k-mers\n",
    "def generate_kmers(sequence, k=4):\n",
    "    kmers = [sequence[i:i+k] for i in range(len(sequence) - k + 1)]\n",
    "    return kmers\n",
    "\n",
    "# Generate k-mers\n",
    "k = 4\n",
    "kmers = generate_kmers(sequence, k)\n",
    "\n",
    "# Create vocabulary of k-mers\n",
    "kmer_counts = Counter(kmers)\n",
    "vocab = list(kmer_counts.keys())\n",
    "char_to_idx = {kmer: idx for idx, kmer in enumerate(vocab)}\n",
    "idx_to_char = {idx: kmer for kmer, idx in char_to_idx.items()}\n",
    "\n",
    "# Encode the sequence of k-mers\n",
    "encoded_seq = np.array([char_to_idx[kmer] for kmer in kmers], dtype=np.int64)\n",
    "\n",
    "# Train-test split\n",
    "split_idx = int(len(encoded_seq) * 0.8)\n",
    "train_data = encoded_seq[:split_idx]\n",
    "test_data = encoded_seq[split_idx:]\n",
    "\n",
    "# ------------------ PyTorch Dataset ------------------\n",
    "class KmerDataset(Dataset):\n",
    "    def __init__(self, data, seq_length):\n",
    "        self.data = data\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.seq_length]\n",
    "        y = self.data[idx + 1:idx + self.seq_length + 1]\n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Parameters\n",
    "seq_length = 50\n",
    "batch_size = 64\n",
    "\n",
    "# DataLoaders\n",
    "train_ds = KmerDataset(train_data, seq_length)\n",
    "test_ds = KmerDataset(test_data, seq_length)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ------------------ Attention RNN Model ------------------\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Linear(hidden_size * 2, 1)  # Changed to output single attention score\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \n",
    "        # Repeat hidden for each time step\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, encoder_outputs.size(1), 1)\n",
    "        \n",
    "        # Concatenate hidden with encoder outputs\n",
    "        energy = torch.cat((hidden, encoder_outputs), dim=2)\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        attention_scores = self.attention(energy).squeeze(2)  # shape: (batch_size, seq_length)\n",
    "        attention_weights = self.softmax(attention_scores)\n",
    "        \n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)  # shape: (batch_size, 1, hidden_size)\n",
    "        \n",
    "        return context.squeeze(1), attention_weights\n",
    "\n",
    "class AttentionLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        \n",
    "        # Encoder LSTM\n",
    "        self.encoder = nn.LSTM(embed_dim, rnn_hidden_size, batch_first=True)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = Attention(rnn_hidden_size)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.LSTM(embed_dim + rnn_hidden_size, rnn_hidden_size, batch_first=True)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        # Initialize hidden state for encoder\n",
    "        h0 = torch.zeros(1, batch_size, self.rnn_hidden_size).to(device)\n",
    "        c0 = torch.zeros(1, batch_size, self.rnn_hidden_size).to(device)\n",
    "        return h0, c0\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)  # shape: (batch, seq_len, embed_dim)\n",
    "        \n",
    "        # Encoder\n",
    "        encoder_outputs, (hidden, cell) = self.encoder(embedded, (hidden, cell))\n",
    "        \n",
    "        # Initialize decoder hidden state\n",
    "        decoder_hidden = hidden\n",
    "        decoder_cell = cell\n",
    "        \n",
    "        # Initialize output tensor\n",
    "        outputs = torch.zeros(x.size(0), x.size(1), self.fc.out_features).to(x.device)\n",
    "        \n",
    "        # Process each time step\n",
    "        for t in range(x.size(1)):\n",
    "            # Get current input\n",
    "            current_input = embedded[:, t:t+1]\n",
    "            \n",
    "            # Apply attention - ensure decoder_hidden is properly squeezed\n",
    "            decoder_hidden_squeezed = decoder_hidden.squeeze(0)  # Remove the num_layers dimension\n",
    "            context, _ = self.attention(decoder_hidden_squeezed, encoder_outputs)\n",
    "            context = context.unsqueeze(1)  # Add back the sequence length dimension\n",
    "            \n",
    "            # Concatenate input with context\n",
    "            decoder_input = torch.cat((current_input, context), dim=2)\n",
    "            \n",
    "            # Decoder step\n",
    "            decoder_output, (decoder_hidden, decoder_cell) = self.decoder(\n",
    "                decoder_input, (decoder_hidden, decoder_cell)\n",
    "            )\n",
    "            \n",
    "            # Output layer\n",
    "            output = self.fc(decoder_output.squeeze(1))\n",
    "            outputs[:, t] = output\n",
    "            \n",
    "        return outputs, hidden, cell\n",
    "\n",
    "# ------------------ Training ------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 256\n",
    "rnn_hidden_size = 512\n",
    "\n",
    "model = AttentionLSTM(vocab_size, embed_dim, rnn_hidden_size).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    hidden, cell = model.init_hidden(batch_size, device)\n",
    "    seq_batch, target_batch = next(iter(train_dl))\n",
    "    seq_batch = seq_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    output, hidden, cell = model(seq_batch, hidden, cell)\n",
    "    \n",
    "    # Reshape for loss\n",
    "    output = output.view(-1, vocab_size)\n",
    "    target_batch = target_batch.view(-1)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = loss_fn(output, target_batch)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# ------------------ Testing ------------------\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for seq_batch, target_batch in test_dl:\n",
    "        seq_batch, target_batch = seq_batch.to(device), target_batch.to(device)\n",
    "        hidden, cell = model.init_hidden(seq_batch.size(0), device)\n",
    "        output, _, _ = model(seq_batch, hidden, cell)\n",
    "        \n",
    "        # Reshape for loss\n",
    "        output = output.view(-1, vocab_size)\n",
    "        target_batch = target_batch.view(-1)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_fn(output, target_batch)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "avg_test_loss = test_loss / len(test_dl)\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "# ------------------ Prediction Example ------------------\n",
    "def predict_next_kmer(model, initial_sequence, k=4, num_predictions=5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Convert initial sequence to k-mers\n",
    "        initial_kmers = generate_kmers(initial_sequence, k)\n",
    "        # Convert to indices\n",
    "        input_indices = [char_to_idx[kmer] for kmer in initial_kmers if kmer in char_to_idx]\n",
    "        \n",
    "        if not input_indices:\n",
    "            return \"No valid k-mers found in initial sequence\"\n",
    "            \n",
    "        # Convert to tensor\n",
    "        input_tensor = torch.tensor(input_indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        hidden, cell = model.init_hidden(1, device)\n",
    "        \n",
    "        predictions = []\n",
    "        for _ in range(num_predictions):\n",
    "            output, hidden, cell = model(input_tensor, hidden, cell)\n",
    "            predicted_idx = output[:, -1].argmax(1).item()\n",
    "            predictions.append(idx_to_char[predicted_idx])\n",
    "            input_tensor = torch.cat([input_tensor, torch.tensor([[predicted_idx]], dtype=torch.long).to(device)], dim=1)\n",
    "            \n",
    "    return predictions\n",
    "\n",
    "# Example usage\n",
    "test_sequence = sequence[:20]  # Take first 20 nucleotides\n",
    "predictions = predict_next_kmer(model, test_sequence)\n",
    "print(\"\\nInitial sequence:\", test_sequence)\n",
    "print(\"Predicted next k-mers:\", predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
